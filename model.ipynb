{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEXpCSS3f1Dq"
      },
      "source": [
        "Example of building a simple CNN for CIFAR10 dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCLtTkn93MT_",
        "outputId": "88643839-cf44-4a84-f0e9-851f40db7f62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yHQe_1T3XVe"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/My Drive/EuroSAT-RGB.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-y-x_Vx6hVJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h_AF4ZZ6fJp"
      },
      "outputs": [],
      "source": [
        "\n",
        "#transforms train & test data \n",
        "\n",
        "transforms_train = transforms.Compose([\n",
        "      \n",
        "    transforms.Resize((64, 64)),   #must same as here\n",
        "    transforms.RandomHorizontalFlip(), # data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # normalization\n",
        "])\n",
        "\n",
        "\n",
        "transforms_test = transforms.Compose([\n",
        "  \n",
        "    transforms.Resize((64, 64)),   #must same as here\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2TCz2oV_QwX",
        "outputId": "abdc6902-e6db-4f6b-c63f-22dedaa73b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n"
          ]
        }
      ],
      "source": [
        "# # Creating Train  / Test folders (One time use)\n",
        "# this  runs once\n",
        "\n",
        "\n",
        "import shutil\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "classes =[ '/River', '/SeaLake','/AnnualCrop','/Forest','/HerbaceousVegetation','/Highway','/Industrial','/Pasture','/PermanentCrop','/Residential']\n",
        "root_dir = 'DataSet01'\n",
        "print(len(classes))\n",
        "index =0\n",
        "while index+1 < len(classes):\n",
        "  posCls = classes[index]\n",
        "  negCls = classes[index+1]\n",
        "  index+=2\n",
        "\n",
        "  os.makedirs(root_dir +'/train' + posCls)\n",
        "  os.makedirs(root_dir +'/train' + negCls)\n",
        "\n",
        "  os.makedirs(root_dir +'/test' + posCls)\n",
        "  os.makedirs(root_dir +'/test' + negCls)\n",
        "\n",
        "  # Creating partitions of the data after shuffeling\n",
        "  currentCls = posCls\n",
        "  src = \"EuroSAT-RGB/2750/\"+currentCls # Folder to copy images from\n",
        "\n",
        "  allFileNames = os.listdir(src)\n",
        "  np.random.shuffle(allFileNames)\n",
        "  train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
        "                                                            [int(len(allFileNames)*0.7), int(len(allFileNames)*0.85)])\n",
        "\n",
        "\n",
        "  train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
        "  test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
        "\n",
        "# Copy-pasting images\n",
        "  for name in train_FileNames:\n",
        "      shutil.copy(name, root_dir+\"/train\"+currentCls)\n",
        "\n",
        "\n",
        "\n",
        "  for name in test_FileNames:\n",
        "      shutil.copy(name, root_dir+\"/test\"+currentCls)\n",
        "  # Creating partitions of the data after shuffeling\n",
        "  currentCls = negCls\n",
        "  src = \"EuroSAT-RGB/2750/\"+currentCls # Folder to copy images from\n",
        "\n",
        "  allFileNames = os.listdir(src)\n",
        "  np.random.shuffle(allFileNames)\n",
        "  # spliting every class with balancing them\n",
        "  train_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
        "                                                            [int(len(allFileNames)*0.7)])\n",
        "  train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
        "  test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
        "\n",
        "  for name in train_FileNames:\n",
        "      shutil.copy(name, root_dir+\"/train\"+currentCls)\n",
        "\n",
        "  for name in test_FileNames:\n",
        "      shutil.copy(name, root_dir+\"/test\"+currentCls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmAG-kFhCd8q",
        "outputId": "b314dfb2-4dbe-4db4-bcf4-4b8bb17976cd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# use GPU if avaliable\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlwwnNokdEsG",
        "outputId": "a04b3ff8-8227-43b5-8ca5-67db8db4d763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0 running\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 0.9387733889666815 tensor(66.3757)\n",
            "Epoch 1 running\n",
            "1 0.6314082072083912 tensor(77.6614)\n",
            "Epoch 2 running\n",
            "2 0.49854683318781473 tensor(82.4762)\n",
            "Epoch 3 running\n",
            "3 0.41817943485719816 tensor(85.5344)\n",
            "Epoch 4 running\n",
            "4 0.3446819563777674 tensor(88.2169)\n"
          ]
        }
      ],
      "source": [
        "''' \n",
        "We import the necessary libraries and modules for our code.\n",
        "We use PyTorch as our deep learning framework, \n",
        "nn module for defining neural networks, \n",
        "optim module for optimization algorithms, \n",
        "and transforms module for data preprocessing.\n",
        "'''\n",
        "\n",
        "\n",
        "# Define the RESNET 18 architecture\n",
        "\n",
        "# Create NN For common part\n",
        "class NN_Class(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n",
        "        super(NN_Class, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.identity_downsample = identity_downsample\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        # conv 1\n",
        "        x = self.conv1(x)\n",
        "        # batch normailztion 1\n",
        "        x = self.bn1(x)\n",
        "        # RELU\n",
        "        x = self.relu(x)\n",
        "        # conv 2\n",
        "        x = self.conv2(x)\n",
        "        # batch normalzaztion 2\n",
        "        x = self.bn2(x)\n",
        "        # identity matix\n",
        "        # do down sampling\n",
        "        if self.identity_downsample is not None:\n",
        "            identity = self.identity_downsample(identity)\n",
        "        x += identity\n",
        "        # RELU\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self,image_channels, num_classes):\n",
        "        \"\"\"\n",
        "        CNN Architecture: 2 convolutional layers, 1 max pool, 2 fully connected layer, \n",
        "        and final fully connected layer to classify 10 different classes\n",
        "        \"\"\"\n",
        "        super(Net, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        # Depp NN layers\n",
        "        self.layer1 = self.create_layer(64, 64, stride=1)\n",
        "        self.layer2 = self.create_layer(64, 128, stride=2)\n",
        "        self.layer3 = self.create_layer(128, 256, stride=2)\n",
        "        self.layer4 = self.create_layer(256, 512, stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "      \n",
        "    def create_layer(self, in_channels, out_channels, stride):\n",
        "        \n",
        "        identity_downsample = None\n",
        "        if stride != 1:\n",
        "            identity_downsample = self.identity_downsample(in_channels, out_channels)\n",
        "            \n",
        "        return nn.Sequential(\n",
        "            NN_Class(in_channels, out_channels, identity_downsample=identity_downsample, stride=stride), \n",
        "            NN_Class(out_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Defines the forward pass of the network.\n",
        "        \"\"\"\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        return x \n",
        "    def identity_downsample(self, in_channels, out_channels):\n",
        "        \n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
        "            nn.BatchNorm2d(out_channels)\n",
        "        )\n",
        "      \n",
        "\n",
        "# Load the dataset\n",
        "\n",
        "\"\"\"\n",
        "Creates a data transformation pipeline using the transforms.Compose function from the PyTorch library.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dir = root_dir+\"/train/\"\n",
        "test_dir = root_dir+\"/test/\"\n",
        "\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transforms_train)\n",
        "test_dataset = ImageFolder(test_dir, transforms_test)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=8)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=12, shuffle=False, num_workers=8)\n",
        "\n",
        "# Train the network\n",
        "model = Net(3, 10)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "num_epochs=5\n",
        "\n",
        "for epoch in range(num_epochs): #(loop for every epoch)\n",
        "    print(\"Epoch {} running\".format(epoch)) #(printing message)\n",
        "    \"\"\" Training Phase \"\"\"\n",
        "    model.train()    #(training model)\n",
        "    running_loss = 0.   #(set loss 0)\n",
        "    running_corrects = 0 \n",
        "    # load a batch data of images\n",
        "    targets = []\n",
        "    results = []\n",
        "    i=0\n",
        "    for inputs,labels in train_dataloader:\n",
        "        i+=1\n",
        "        outputs = model(inputs)        \n",
        "        optimizer.zero_grad()\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)\n",
        "        results.append(preds)\n",
        "        targets.append(labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    results =torch.cat(results, dim=0)\n",
        "    targets = torch.cat(targets, dim=0)\n",
        "\n",
        "    results = results.to('cpu').numpy().flatten()\n",
        "    targets = targets.to('cpu').numpy().flatten()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    epoch_acc = running_corrects / len(train_dataset) * 100.\n",
        "    print(epoch, epoch_loss, epoch_acc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTFKv5Xn-WYv",
        "outputId": "bea05a4c-0625-42ef-d917-0141792fff77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training acc tensor(88.2169)\n"
          ]
        }
      ],
      "source": [
        "    print('Training acc', epoch_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kaqtWWNFCbv",
        "outputId": "910214a3-4314-4755-e71e-ed7150add7ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accu tensor(88.2305)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "    \"\"\" Test Phase \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        running_loss = 0.\n",
        "        running_corrects = 0\n",
        "        targets = []\n",
        "        results = []\n",
        "        for inputs, labels in test_dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            results.append(preds)\n",
        "            targets.append(labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "        epoch_loss = running_loss / len(test_dataset)\n",
        "        epoch_acc = running_corrects / len(test_dataset) * 100.\n",
        "        results = torch.cat(results, dim=0)\n",
        "        targets =torch.cat(targets, dim=0)\n",
        "        results = results.to('cpu').numpy().flatten()\n",
        "        targets = targets.to('cpu').numpy().flatten()\n",
        "       \n",
        "        print(\"Test Accu\",epoch_acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
